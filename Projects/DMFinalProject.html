<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Music Project – Audio Features & Song Popularity</title>
</head>
<body>
  <h1>Final Project: Music Popularity Prediction</h1>
  <p> By: Griffin Nixdorf, Unique Davis, Tim Abbott, Brian Cruz, Faith Madukwe, Lavashia Crump</p>

  
  <h2>Introduction</h2>
<p>
  What made a song popular in the 80s? There are many different complex factors, such as artistic values like rhythm, danceability, liveness, and many more, along with consumer behavior trends. Now that streaming platforms like Spotify collect comprehensive data on song characteristics and trends, we can track audio features like tempo, danceability, and energy levels to see if a song will become a hit. Our goal is to analyze these features and make a predictive model to see if a song made in the 80s will become a hit or not. 
</p>
  
  <h2>About the Data</h2>
  <p>
  We are using this dataset: 
  <a href="https://www.kaggle.com/datasets/theoverman/the-spotify-hit-predictor-dataset" target="_blank">
    The Spotify Hit Predictor Dataset (1960-2019)</a>
  </p>
  <p>The dataset consists of different tracks from the 60s to 2019. We have decided to look specifically into the 80s for our project. Each song has a target value of 0-1, 0 indicating the song is a flop and 1 indicating the song is a hit. Additionally, each song has several audio features, such as:</p>

<table border="1">
  <thead>
    <tr>
      <th>Feature</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>track</td><td>Name of the track.</td></tr>
    <tr><td>artist</td><td>Name of the artist.</td></tr>
    <tr><td>uri</td><td>Spotify resource ID for the track.</td></tr>
    <tr><td>danceability</td><td>How suitable the track is for dancing (0.0–1.0).</td></tr>
    <tr><td>energy</td><td>Intensity and activity level of the track (0.0–1.0).</td></tr>
    <tr><td>key</td><td>Musical key of the track (e.g., 0 = C, 1 = C♯/D♭).</td></tr>
    <tr><td>loudness</td><td>Average loudness in decibels (dB).</td></tr>
    <tr><td>mode</td><td>1 = major, 0 = minor.</td></tr>
    <tr><td>speechiness</td><td>Presence of spoken words (0.0–1.0).</td></tr>
    <tr><td>acousticness</td><td>Likelihood of being an acoustic track (0.0–1.0).</td></tr>
    <tr><td>instrumentalness</td><td>Likelihood the track has no vocals (0.0–1.0).</td></tr>
    <tr><td>liveness</td><td>Likelihood the track is live (0.0–1.0).</td></tr>
    <tr><td>valence</td><td>Musical positivity (0.0–1.0).</td></tr>
    <tr><td>tempo</td><td>Tempo in beats per minute (BPM).</td></tr>
    <tr><td>duration_ms</td><td>Duration of the track in milliseconds.</td></tr>
    <tr><td>time_signature</td><td>Estimated time signature (e.g., 4 = 4/4).</td></tr>
    <tr><td>chorus_hit</td><td>Estimated timestamp of chorus start.</td></tr>
    <tr><td>sections</td><td>Number of sections in the track.</td></tr>
    <tr><td>target</td><td>1 = hit, 0 = flop (based on Billboard Hot-100).</td></tr>
  </tbody>
</table>


  <p>
    The 80s song dataset has 6908 unique songs, along with 19 different columns that will allow us to explore trends through different modeling techniques.
  </p>
  <h2>Methods</h2>
  <p>
    Before conducting any analysis, we began by preparing the dataset to ensure consistency and reliability. Our focus was specifically on songs from the 1980s, so we filtered the Spotify Hit Predictor dataset accordingly, resulting in a final set of 6,908 songs. We then checked for missing values and found none, confirming that the dataset was complete. Additionally, we examined the class distribution of our target variable, whether a song was a hit or a flop, and found it to be perfectly balanced, with 3,454 songs in each category. This balance is ideal for training a binary classification model without introducing bias.
  </p>
  <p>
    To streamline our analysis, we dropped non-predictive columns such as track, artist, and URI, which contain text-based identifiers rather than useful numerical features. Next, we standardized the remaining numerical features using StandardScaler, ensuring that each feature was on the same scale and that no single variable would dominate the model due to differing units or ranges. We also created boxplots for each audio feature to visually inspect distributions and identify any potential outliers or skewness.
  </p>
  <p>
    In order to better understand relationships between variables, we generated a correlation matrix and examined the strength of each feature's association with the target variable. Finally, we calculated the Variance Inflation Factor (VIF) to check for multicollinearity among features. Most features fell within an acceptable range, although duration_ms and sections showed moderately elevated VIF scores, indicating some correlation with other variables. These pre-processing steps provided a solid foundation for further exploration and model development by ensuring that the data was clean, well-structured, and ready for analysis.
  </p>
  <img src="../docs/preheatmap.png" alt="1"width="700"> <img src="../docs/preboxplot.png" alt="1"width="800">
  
  <p>We are approaching the problem as a binary classification task (<strong>Hit</strong> vs. <strong>Flop</strong>) using one of two supervised learning models:</p>
  
  <ul>
    <li><strong>Logistic Regression</strong>: A good baseline model for binary classification.</li>
    <li><strong>Random Forest</strong>: To capture non-linear relationships and identify important features.</li>
  </ul>
  
  <p>In addition, we’ll perform <strong>feature importance analysis</strong> (using permutation importance or model-specific methods) to determine which features contribute most to a song’s popularity. This could be insightful for understanding musical trends and knowing which features will be most useful to include in our model.</p>
  
  <h2>Evaluation</h2>
  <p>Our evaluation will focus on measuring the performance of our classification models in predicting whether a song is a “hit” or a “flop” based on its audio features. We will use the following classification metrics to evaluate our models:</p>
  
  <ul>
    <li><strong>Accuracy</strong> – Measures the rate of correct predictions.</li>
    <li><strong>Precision</strong> – Evaluates the proportion of correctly predicted hits among all predicted hits.</li>
    <li><strong>Recall</strong> – Assesses the ability of the model to identify hits.</li>
    <li><strong>F1 Score</strong> – Balances precision and recall into a single metric.</li>
  </ul>

  <p>A <strong>confusion matrix</strong> will be used to visualize model performance across all of the prediction categories.</p>
    <img src="../docs/confmatrix.png" alt="1"width="600"> <img src="../docs/graph.png" alt="1"width="800">

    <img src="../docs/download.png" alt="1"width="600">

  <h2>Storytelling and Conclusion</h2>
  <p>
    Not done yet.
  </p>

  <h2>Impact Section</h2>
  <p>
    This project can help influence artists, producers, and labels to approach music with data-driven insights. If artists start using our model to predict what makes a song a hit, they might focus more on those traits and create more songs that are likely to become hits. Our project shows how patterns in music can be found using data science, bridging the gap between music and technology in a meaningful way.
  </p>

  <p>
    But there’s a downside. If artists, producers, or labels start following the same patterns just to make hit songs, music could lose originality and start to sound the same. Artists who want to do something different might be pushed aside or have fewer opportunities. They might even feel pressured to tailor their work to what the algorithm prefers—especially if labels care more about profit than creativity.
  </p>

  <p>
    Over time, listeners could end up hearing the same kinds of songs again and again, making music less diverse. People might be less exposed to different sounds, genres, and cultures. Also, since our model is trained on data from the 1980s, it might carry bias from that time, favoring mainstream genres and overlooking newer or more diverse styles of music. This bias could limit what gets produced and promoted, shaping the future of music in a way that’s less inclusive and less innovative.
  </p>
  <h2>Code</h2>
  <p>
    The full code is available below:
  </p>
  <iframe src="https://nbviewer.org/github/griffin-nixdorf/ITSC3162-Final-Project/blob/main/Data_Mining_Final_Modeling.ipynb" width="100%" height="600px"></iframe>
  <iframe src="https://nbviewer.org/github/griffin-nixdorf/ITSC3162-Final-Project/blob/main/Data_Mining_Final_Logistic%20Regression.ipynb" width="100%" height="600px"></iframe>

  <h2>References</h2>
  <ul>
    <li><a href="https://www.kaggle.com/datasets/theoverman/the-spotify-hit-predictor-dataset" target="_blank">The Spotify Hit Predictor Dataset (1960-2019) - Kaggle</a></li>
  </ul>
</body>
</html>
