<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Music Project – Audio Features & Song Popularity</title>
</head>
<body>
  <h1>Final Project: Music Popularity Prediction</h1>
  <p> By: Griffin Nixdorf, Unique Davis, Tim Abbott, Brian Cruz, Faith Madukwe, Lavashia Crump</p>

  
  <h2>Introduction</h2>
<p>
  What made a song popular in the 80s? There are many different complex factors, such as artistic values like rhythm, danceability, liveness, and many more, along with consumer behavior trends. Now that streaming platforms like Spotify collect comprehensive data on song characteristics and trends, we can track audio features like tempo, danceability, and energy levels to see if a song will become a hit. Our goal is to analyze these features and make a predictive model to see if a song made in the 80s will become a hit or not. 
</p>
  
  <h2>About the Data</h2>
  <p>
  We are using this dataset: 
  <a href="https://www.kaggle.com/datasets/theoverman/the-spotify-hit-predictor-dataset" target="_blank">
    The Spotify Hit Predictor Dataset (1960-2019)</a>
  </p>
  <p>The dataset consists of different tracks from the 60s to 2019. We have decided to look specifically into the 80s for our project. Each song has a target value of 0-1, 0 indicating the song is a flop and 1 indicating the song is a hit. Additionally, each song has several audio features, such as:</p>

<table border="1">
  <thead>
    <tr>
      <th>Feature</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>track</td><td>Name of the track.</td></tr>
    <tr><td>artist</td><td>Name of the artist.</td></tr>
    <tr><td>uri</td><td>Spotify resource ID for the track.</td></tr>
    <tr><td>danceability</td><td>How suitable the track is for dancing (0.0–1.0).</td></tr>
    <tr><td>energy</td><td>Intensity and activity level of the track (0.0–1.0).</td></tr>
    <tr><td>key</td><td>Musical key of the track (e.g., 0 = C, 1 = C♯/D♭).</td></tr>
    <tr><td>loudness</td><td>Average loudness in decibels (dB).</td></tr>
    <tr><td>mode</td><td>1 = major, 0 = minor.</td></tr>
    <tr><td>speechiness</td><td>Presence of spoken words (0.0–1.0).</td></tr>
    <tr><td>acousticness</td><td>Likelihood of being an acoustic track (0.0–1.0).</td></tr>
    <tr><td>instrumentalness</td><td>Likelihood the track has no vocals (0.0–1.0).</td></tr>
    <tr><td>liveness</td><td>Likelihood the track is live (0.0–1.0).</td></tr>
    <tr><td>valence</td><td>Musical positivity (0.0–1.0).</td></tr>
    <tr><td>tempo</td><td>Tempo in beats per minute (BPM).</td></tr>
    <tr><td>duration_ms</td><td>Duration of the track in milliseconds.</td></tr>
    <tr><td>time_signature</td><td>Estimated time signature (e.g., 4 = 4/4).</td></tr>
    <tr><td>chorus_hit</td><td>Estimated timestamp of chorus start.</td></tr>
    <tr><td>sections</td><td>Number of sections in the track.</td></tr>
    <tr><td>target</td><td>1 = hit, 0 = flop (based on Billboard Hot-100).</td></tr>
  </tbody>
</table>


  <p>
    The 80s song dataset has 6908 unique songs, along with 19 different columns that will allow us to explore trends through different modeling techniques.
  </p>
  <h2>Methods</h2>
  <p>
    Before conducting any analysis, we began by preparing the dataset to ensure consistency and reliability. Our focus was specifically on songs from the 1980s, so we filtered the Spotify Hit Predictor dataset accordingly, resulting in a final set of 6,908 songs. We then checked for missing values and found none, confirming that the dataset was complete. Additionally, we examined the class distribution of our target variable, whether a song was a hit or a flop, and found it to be perfectly balanced, with 3,454 songs in each category. This balance is ideal for training a binary classification model without introducing bias.
  </p>
  <p>
    To streamline our analysis, we dropped non-predictive columns such as track, artist, and URI, which contain text-based identifiers rather than useful numerical features. Next, we standardized the remaining numerical features using StandardScaler, ensuring that each feature was on the same scale and that no single variable would dominate the model due to differing units or ranges. We also created boxplots for each audio feature to visually inspect distributions and identify any potential outliers or skewness.
  </p>
  <p>
    In order to better understand relationships between variables, we generated a correlation matrix and examined the strength of each feature's association with the target variable. Finally, we calculated the Variance Inflation Factor (VIF) to check for multicollinearity among features. Most features fell within an acceptable range, although duration_ms and sections showed moderately elevated VIF scores, indicating some correlation with other variables. These pre-processing steps provided a solid foundation for further exploration and model development by ensuring that the data was clean, well-structured, and ready for analysis.
  </p>
  <img src="../docs/preheatmap.png" alt="1"width="700"> <img src="../docs/preboxplot.png" alt="1"width="800">
  
  <p>We approached the problem as a binary classification task (<strong>Hit</strong> vs. <strong>Flop</strong>) using two supervised learning models:</p>
  
  <ul>
    <li><strong>Logistic Regression Model</strong>: A good baseline model for binary classification.</li>
    <li><strong>Random Forest Model</strong>: To capture non-linear relationships and identify important features.</li>
  </ul>
  
  <p>Additionally, we performed <strong>feature importance analysis</strong> (using permutation importance or model-specific methods) to determine which features contribute most to a song’s popularity.</p>

  <h2>Evaluation</h2>
  <p>Our evaluation focuses on measuring the performance of our classification models in predicting whether a song is a “hit” or a “flop” based on its audio features. We are using the following classification metrics to evaluate our models:</p>
  
  <ul>
    <li><strong>Accuracy</strong> – Measures the rate of correct predictions.</li>
    <li><strong>Precision</strong> – Evaluates the proportion of correctly predicted hits among all predicted hits.</li>
    <li><strong>Recall</strong> – Assesses the ability of the model to identify hits.</li>
    <li><strong>F1 Score</strong> – Balances precision and recall into a single metric.</li>
  </ul>

  <p>A <strong>confusion matrix</strong> will be used to visualize model performance across all of the prediction categories.</p>

    <h3>Logistic Regression Model</h3>
    <p>
        After preprocessing, we split the data: 75% to train the model and 25% to test it, using the same random seed each time to ensure identical results for all group members. A scikit-learn pipeline was used to scale the features and fit a logistic regression model with class-balanced weights. On the test set, it achieved 76% accuracy, 72% precision, 84% recall, and an F1 score of 0.78 (correctly identifying 726 hits and 586 flops). The learned weights showed that more danceable, louder, and happier (higher valence) songs are more likely to be hits, while instrumental or speech-like tracks tended to flop.
    </p>
    <p>
        This logistic model provides a simple, interpretable starting point and identifies key audio traits associated with hit songs. The trained model and its coefficient table were saved for quick reuse, allowing anyone to load them later without retraining the model.
    </p>
    
    <img src="../docs/download.png" alt="1"width="600">

    <h3>Random Forest Model</h3>
    <p>
        Next, we employed a Random Forest classifier to predict whether a song from the 1980s would be a hit or a flop based on its audio features. After training the model on a balanced dataset, we evaluated its performance using standard classification metrics. The model performed well, especially in identifying hit songs, producing strong scores across accuracy, precision, recall, and F1. We also examined the confusion matrix to better understand the model's errors.
    </p>
    <p>
        Our Random Forest model achieved an accuracy of 81.2% and an F1 score of 0.79. The confusion matrix revealed that the model was better at identifying hits than flops. Feature importance analysis showed that factors like danceability, energy, and valence were key predictors. We also tested a tuned version of the model with more trees and class balancing, but it did not improve performance, indicating the original configuration was already well-suited to the data.
    </p>
  
    <img src="../docs/confmatrix.png" alt="1"width="600"> <img src="../docs/graph.png" alt="1"width="800">

  <h2>Storytelling and Conclusion</h2>
  <p>
    Artists and music labels in the 80s had to rely on feelings and trending songs to potentially craft hits. Looking through the modern lens by utilizing resources such as Spotify audio feature data to ask questions like, “Could we have predicted the next generational hit based on measured song traits? ” Which leads to describing how our data collection may answer this question. We filtered 6,908 songs to confirm the balance between the hits and flops of that era, distilling the tracks to their core features such as danceability, energy, valence, etc. From there, we weave in our model development narrative—first with logistic regression as a transparent baseline, then with Random Forests to capture more complex, non-linear patterns. This leads us to discover moments of insight (e.g., discovering that high danceability and valence consistently mark hits ) and challenges (e.g., navigating multicollinearity in features like duration_ms and sections). This storyline highlights how data science can recreate—and even anticipate—the creative decisions that drove ‘80s pop culture.
  </p>

  <p>
    In conclusion, our analyses showcase that a song’s measurable audio traits can reliably forecast its success. The Random Forest model reached 81.2% accuracy and scored an F1 score of 0.79, outperforming the logistic baseline. Through key predictors such as danceability, valence, and energy, it closely aligns with what ‘80s audiences loved, with upbeat, emotionally positive, and rhythmically engaging tracks. The results validate the promise of data-driven decision-making in music production, offering a quantifiable supplement to artistic intuition. However, there are risks, such as over-optimizing these features, that could homogenize music and limit creativity. Next, integrating lyrical sentiment analysis, testing on other decades, and employing interpretability tools like SHAP will refine our predictions and safeguard against unintended bias. Finally, our project links gaps between art and analytics, showing how data science can honor musical heritage while guiding the next generation of hitmakers.
  </p>

  <h2>Impact Section</h2>
  <p>
    This project can help influence artists, producers, and labels to approach music with data-driven insights. If artists start using our model to predict what makes a song a hit, they might focus more on those traits and create more songs that are likely to become hits. Our project shows how patterns in music can be found using data science, bridging the gap between music and technology in a meaningful way.
  </p>

  <p>
    But there’s a downside. If artists, producers, or labels start following the same patterns just to make hit songs, music could lose originality and start to sound the same. Artists who want to do something different might be pushed aside or have fewer opportunities. They might even feel pressured to tailor their work to what the algorithm prefers—especially if labels care more about profit than creativity.
  </p>

  <p>
    Over time, listeners could end up hearing the same kinds of songs again and again, making music less diverse. People might be less exposed to different sounds, genres, and cultures. Also, since our model is trained on data from the 1980s, it might carry bias from that time, favoring mainstream genres and overlooking newer or more diverse styles of music. This bias could limit what gets produced and promoted, shaping the future of music in a way that’s less inclusive and less innovative.
  </p>
  <h2>Code</h2>
  <p>
    The full code is available below:
  </p>
  <iframe src="https://nbviewer.org/github/griffin-nixdorf/ITSC3162-Final-Project/blob/main/Data_Mining_Final_Modeling.ipynb" width="100%" height="600px"></iframe>
  <iframe src="https://nbviewer.org/github/griffin-nixdorf/ITSC3162-Final-Project/blob/main/Data_Mining_Final_Logistic%20Regression.ipynb" width="100%" height="600px"></iframe>

  <h2>References</h2>
  <ul>
    <li><a href="https://www.kaggle.com/datasets/theoverman/the-spotify-hit-predictor-dataset" target="_blank">The Spotify Hit Predictor Dataset (1960-2019) - Kaggle</a></li>
  </ul>
</body>
</html>
